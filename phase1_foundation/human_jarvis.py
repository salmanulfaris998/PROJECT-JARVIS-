"""
Improved Production-Ready JARVIS System
Fixed accuracy issues and enhanced functionality
Version: 2.1 - Accuracy Enhanced with TARS Voice Integration
"""

import asyncio
import time
import json
import os
import sys
import logging
import hashlib
import secrets
from typing import Dict, List, Optional, Any, Union
from datetime import datetime, timedelta
from enum import Enum
from dataclasses import dataclass, asdict
from pathlib import Path
import signal
import threading
import subprocess
import platform
import re
import sqlite3
from contextlib import asynccontextmanager
import requests

# Configure logging first
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('jarvis_improved.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('JARVIS')

# Try to import TARS voice module after logger is configured
try:
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'voice_cloning'))
    from tars_voice_player import TARSVoice
    TARS_VOICE_AVAILABLE = True
    logger.info("TARS voice module successfully imported")
except ImportError:
    TARS_VOICE_AVAILABLE = False
    logger.warning("TARS voice module not available")
    
    # Create a mock TARSVoice class for fallback
    class TARSVoice:
        def __init__(self):
            pass
        
        def speak(self, text):
            print(f"üé≠ TARS (Mock): {text}")
            return False
except Exception as e:
    TARS_VOICE_AVAILABLE = False
    logger.error(f"Error importing TARS voice module: {e}")
    
    # Create a mock TARSVoice class for fallback
    class TARSVoice:
        def __init__(self):
            pass
        
        def speak(self, text):
            print(f"üé≠ TARS (Error): {text}")
            return False

# Try to import optional dependencies
try:
    import speech_recognition as sr
    SPEECH_AVAILABLE = True
except ImportError:
    SPEECH_AVAILABLE = False
    logger.warning("Speech recognition not available")

try:
    import pyttsx3
    TTS_AVAILABLE = True
except ImportError:
    TTS_AVAILABLE = False
    logger.warning("Text-to-speech not available")

try:
    import psutil
    SYSTEM_MONITOR_AVAILABLE = True
except ImportError:
    SYSTEM_MONITOR_AVAILABLE = False
    logger.warning("System monitoring not available")

try:
    import GPUtil
    GPU_MONITOR_AVAILABLE = True
except ImportError:
    GPU_MONITOR_AVAILABLE = False

from cryptography.fernet import Fernet

class EmotionalState(Enum):
    """Enhanced emotional states"""
    NEUTRAL = "neutral"
    HAPPY = "happy"
    EXCITED = "excited"
    FOCUSED = "focused"
    CONCERNED = "concerned"
    AMUSED = "amused"
    CARING = "caring"
    CONFIDENT = "confident"
    THOUGHTFUL = "thoughtful"
    SURPRISED = "surprised"
    EMPATHETIC = "empathetic"
    ANALYTICAL = "analytical"
    CURIOUS = "curious"
    REASSURING = "reassuring"
    APOLOGETIC = "apologetic"
    FRIENDLY = "friendly"
    HELPFUL = "helpful"

class IntentType(Enum):
    """Comprehensive intent classification"""
    CONVERSATION = "conversation"
    TIME_QUERY = "time_query"
    IDENTITY_QUERY = "identity_query"
    SYSTEM_STATUS = "system_status"
    WEATHER_QUERY = "weather_query"
    PHONE_CONTROL = "phone_control"
    SYSTEM_CONTROL = "system_control"
    INFORMATION_QUERY = "information_query"
    TASK_AUTOMATION = "task_automation"
    ENTERTAINMENT = "entertainment"
    PRODUCTIVITY = "productivity"
    HEALTH_WELLNESS = "health_wellness"
    SMART_HOME = "smart_home"
    SECURITY = "security"
    CALENDAR_SCHEDULE = "calendar_schedule"
    EMAIL_COMMUNICATION = "email_communication"
    FILE_MANAGEMENT = "file_management"
    EMERGENCY = "emergency"
    EXIT_COMMAND = "exit_command"

class SecurityLevel(Enum):
    """Security clearance levels"""
    PUBLIC = 1
    PROTECTED = 2
    CONFIDENTIAL = 3
    SECRET = 4
    TOP_SECRET = 5

@dataclass
class AIResponse:
    """Enhanced AI response with metadata"""
    text: str
    emotion: EmotionalState
    intent: IntentType
    confidence: float
    context_data: Dict[str, Any]
    action_required: bool = False
    security_level: SecurityLevel = SecurityLevel.PUBLIC
    processing_time: float = 0.0
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

class HumanJARVIS:
    """JARVIS with TARS voice integration"""
    def __init__(self):
        try:
            if TARS_VOICE_AVAILABLE:
                self.tars_voice = TARSVoice()
                self.tars_enabled = True
                logger.info("HumanJARVIS initialized with TARS voice module.")
            else:
                self.tars_voice = TARSVoice()  # Mock class
                self.tars_enabled = False
                logger.warning("TARS voice not available - using mock fallback.")
        except Exception as e:
            logger.error(f"Failed to initialize TARS voice: {e}")
            self.tars_voice = TARSVoice()  # Mock class
            self.tars_enabled = False

    def get_time(self):
        """Get current time"""
        from datetime import datetime
        now = datetime.now().strftime("%I:%M %p")
        return f"The current time is {now}."

    def respond_to_query(self, query):
        """Basic query response"""
        if "time" in query.lower():
            return self.get_time()
        return "I'm processing your request with enhanced voice capability."

    def speak(self, text):
        """Speak using TARS voice if available"""
        if self.tars_enabled and TARS_VOICE_AVAILABLE:
            try:
                self.tars_voice.speak(text)
                return True
            except Exception as e:
                logger.error(f"TARS voice error: {e}")
                print(f"ü§ñ JARVIS (TARS Error): {text}")
                return False
        else:
            print(f"ü§ñ JARVIS (Text): {text}")
            return False

    def process_input(self, input_text):
        """Process input and respond"""
        response = self.respond_to_query(input_text)
        self.speak(response)
        return response

class ImprovedNLPProcessor:
    """Improved Natural Language Processing with better accuracy"""
    
    def __init__(self):
        self.intent_patterns = {
            IntentType.TIME_QUERY: [
                r'(what|whats)\s+(is\s+)?(the\s+)?time(\s+now|\s+today)?',
                r'(current\s+time|time\s+now)',
                r'(tell\s+me\s+the\s+time)',
                r'(what\s+time\s+is\s+it)'
            ],
            IntentType.IDENTITY_QUERY: [
                r'(who\s+are\s+you|what\s+are\s+you)',
                r'(introduce\s+yourself)',
                r'(tell\s+me\s+about\s+yourself)',
                r'(what\s+is\s+your\s+name)',
                r'(who\s+is\s+jarvis)'
            ],
            IntentType.SYSTEM_STATUS: [
                r'(system\s+status|hardware\s+details)',
                r'(show\s+me\s+my\s+hardware)',
                r'(computer\s+specs|system\s+info)',
                r'(check\s+system|system\s+health)',
                r'(hardware\s+information)'
            ],
            IntentType.WEATHER_QUERY: [
                r'(weather\s+today|todays\s+weather)',
                r'(whats\s+the\s+weather)',
                r'(weather\s+forecast)',
                r'(temperature\s+today)',
                r'(how\s+is\s+the\s+weather)'
            ],
            IntentType.EXIT_COMMAND: [
                r'(exit\s+voice\s+mode)',
                r'(quit\s+voice\s+mode)',
                r'(stop\s+voice\s+mode)',
                r'(return\s+to\s+text)',
                r'(leave\s+voice\s+mode)'
            ],
            IntentType.PHONE_CONTROL: [
                r'(call|phone|dial)',
                r'(text|message|sms)',
                r'(screenshot|capture|photo)',
                r'(unlock|lock)\s+(phone|device)'
            ],
            IntentType.SYSTEM_CONTROL: [
                r'(volume\s+(up|down|mute))',
                r'(brightness\s+(up|down))',
                r'(open|launch)\s+\w+',
                r'(shutdown|restart|reboot)',
                r'(close|quit)\s+\w+'
            ],
            IntentType.EMERGENCY: [
                r'(emergency|help|urgent|critical)',
                r'(call\s+911|call\s+emergency)',
                r'(danger|threat|attack)',
                r'(medical\s+emergency)'
            ]
        }
        
        self.emotion_keywords = {
            EmotionalState.HAPPY: ['good', 'great', 'awesome', 'wonderful', 'excellent', 'fantastic'],
            EmotionalState.EXCITED: ['excited', 'thrilled', 'wow', 'amazing'],
            EmotionalState.CONCERNED: ['worried', 'problem', 'issue', 'trouble', 'error'],
            EmotionalState.CURIOUS: ['curious', 'wondering', 'what', 'how', 'why'],
            EmotionalState.EMPATHETIC: ['sorry', 'sad', 'difficult', 'hard']
        }
    
    def extract_intent(self, text: str) -> tuple[IntentType, float]:
        """Extract intent with improved accuracy"""
        text_lower = text.lower().strip()
        
        # Check for emergency first (highest priority)
        for pattern in self.intent_patterns.get(IntentType.EMERGENCY, []):
            if re.search(pattern, text_lower):
                return IntentType.EMERGENCY, 0.95
        
        # Check for exit commands
        for pattern in self.intent_patterns.get(IntentType.EXIT_COMMAND, []):
            if re.search(pattern, text_lower):
                return IntentType.EXIT_COMMAND, 0.95
        
        # Check other specific intents
        intent_scores = {}
        for intent_type, patterns in self.intent_patterns.items():
            if intent_type in [IntentType.EMERGENCY, IntentType.EXIT_COMMAND]:
                continue
                
            score = 0
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    score += 0.8  # Higher base score for pattern matches
            
            if score > 0:
                intent_scores[intent_type] = min(score, 0.95)
        
        if intent_scores:
            best_intent = max(intent_scores, key=intent_scores.get)
            return best_intent, intent_scores[best_intent]
        
        return IntentType.CONVERSATION, 0.7
    
    def extract_emotion(self, text: str) -> EmotionalState:
        """Extract emotional context"""
        text_lower = text.lower()
        
        emotion_scores = {}
        for emotion, keywords in self.emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                emotion_scores[emotion] = score
        
        if emotion_scores:
            return max(emotion_scores, key=emotion_scores.get)
        
        # Check punctuation
        if '!' in text:
            return EmotionalState.EXCITED
        elif '?' in text:
            return EmotionalState.CURIOUS
        
        return EmotionalState.NEUTRAL

class ImprovedVoiceSystem:
    """Improved voice system with TARS voice integration and better error handling"""
    
    def __init__(self):
        self.tts_engine = None
        self.recognizer = None
        self.microphone = None
        self.voice_enabled = False
        self.human_jarvis = HumanJARVIS()
        
        self._initialize_tts()
        self._initialize_stt()
    
    def _initialize_tts(self):
        """Initialize text-to-speech with TARS voice priority"""
        # Try TARS voice first
        if self.human_jarvis.tars_enabled:
            logger.info("TARS voice system initialized as primary TTS")
            return
        
        # Fallback to standard TTS
        if TTS_AVAILABLE:
            try:
                self.tts_engine = pyttsx3.init()
                self.tts_engine.setProperty('rate', 200)
                self.tts_engine.setProperty('volume', 0.8)
                logger.info("Standard TTS system initialized as fallback")
            except Exception as e:
                logger.error(f"TTS initialization failed: {e}")
        else:
            logger.warning("No TTS available - install pyttsx3 or fix TARS voice")
    
    def _initialize_stt(self):
        """Initialize speech-to-text"""
        if SPEECH_AVAILABLE:
            try:
                self.recognizer = sr.Recognizer()
                self.microphone = sr.Microphone()
                
                with self.microphone as source:
                    self.recognizer.adjust_for_ambient_noise(source, duration=1)
                
                self.voice_enabled = True
                logger.info("STT system initialized")
            except Exception as e:
                logger.error(f"STT initialization failed: {e}")
                self.voice_enabled = False
        else:
            logger.warning("STT not available - install SpeechRecognition and pyaudio")
    
    async def speak(self, text: str, emotion: EmotionalState = EmotionalState.NEUTRAL) -> bool:
        """Text-to-speech with TARS voice priority and emotion"""
        # Try TARS voice first
        if self.human_jarvis.tars_enabled:
            try:
                return self.human_jarvis.speak(text)
            except Exception as e:
                logger.error(f"TARS voice failed, using fallback: {e}")
        
        # Fallback to standard TTS
        if not self.tts_engine:
            print(f"ü§ñ JARVIS: {text}")
            return True
        
        try:
            # Adjust voice based on emotion
            rate = 200
            volume = 0.8
            
            if emotion == EmotionalState.EXCITED:
                rate = 220
                volume = 0.9
            elif emotion == EmotionalState.CONCERNED:
                rate = 180
                volume = 0.7
            elif emotion == EmotionalState.CARING:
                rate = 190
                volume = 0.75
            
            self.tts_engine.setProperty('rate', rate)
            self.tts_engine.setProperty('volume', volume)
            
            def speak_text():
                self.tts_engine.say(text)
                self.tts_engine.runAndWait()
            
            # Run in executor to avoid blocking
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, speak_text)
            
            return True
            
        except Exception as e:
            logger.error(f"TTS error: {e}")
            print(f"ü§ñ JARVIS: {text}")
            return False
    
    async def listen(self, timeout: int = 5) -> Optional[str]:
        """Speech recognition with timeout"""
        if not self.voice_enabled or not self.microphone:
            return None
        
        try:
            with self.microphone as source:
                logger.info("Listening for speech...")
                audio = self.recognizer.listen(source, timeout=timeout, phrase_time_limit=8)
            
            # Use Google's speech recognition
            text = self.recognizer.recognize_google(audio)
            logger.info(f"Recognized: {text}")
            return text
            
        except sr.WaitTimeoutError:
            logger.info("Listening timeout")
            return None
        except sr.UnknownValueError:
            logger.info("Could not understand audio")
            return None
        except sr.RequestError as e:
            logger.error(f"Speech recognition error: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected speech error: {e}")
            return None

class AccurateAIBrain:
    """AI brain with improved accuracy and response generation"""
    
    def __init__(self, data_dir: Path):
        self.data_dir = data_dir
        self.nlp = ImprovedNLPProcessor()
        self.conversation_memory = []
        self.user_preferences = {}
        self.human_jarvis = HumanJARVIS()
        
        # Create user info storage
        self.user_info = {
            'name': 'Sir',  # Default address
            'preferences': {},
            'last_interaction': None
        }
        
        self.personality_responses = {
            'greeting': [
                "Good day, Sir! I am JARVIS, your advanced AI assistant with TARS voice capability. How may I serve you today?",
                "Hello! JARVIS at your service, now featuring enhanced TARS voice technology, ready to assist with any task or inquiry.",
                "Greetings, Sir! Your intelligent assistant JARVIS is online and operational with improved voice synthesis."
            ],
            'identity': [
                "I am JARVIS - Just A Rather Very Intelligent System. I'm your advanced AI assistant with TARS voice integration, designed to help with various tasks, provide information, and control connected devices.",
                "I'm JARVIS, your artificial intelligence assistant with enhanced TARS voice capability. Think of me as your digital companion, ready to help with questions, tasks, and system control.",
                "JARVIS stands for Just A Rather Very Intelligent System. I'm here to assist you with information, device control, and general conversation, now with improved voice synthesis technology."
            ],
            'time_response': [
                f"The current time is {datetime.now().strftime('%I:%M %p')} on {datetime.now().strftime('%A, %B %d, %Y')}.",
                f"It's currently {datetime.now().strftime('%I:%M %p')}, {datetime.now().strftime('%A')}.",
                f"The time right now is {datetime.now().strftime('%I:%M %p')} on this {datetime.now().strftime('%A')}."
            ],
            'weather_response': [
                "I don't currently have access to live weather data, but I can help you check the weather through your preferred weather app or website.",
                "For accurate weather information, I recommend checking your local weather service or app. I can help you open a weather application if you'd like.",
                "I'd be happy to help you get weather information. Would you like me to open a weather app or direct you to a weather website?"
            ]
        }
    
    async def process_input(self, user_input: str, user_context: Dict[str, Any] = None) -> AIResponse:
        """Process input with improved accuracy"""
        start_time = time.time()
        
        if user_context is None:
            user_context = {}
        
        try:
            # Extract intent and emotion with improved accuracy
            intent, confidence = self.nlp.extract_intent(user_input)
            emotion = self.nlp.extract_emotion(user_input)
            
            # Generate context-aware response
            context_data = {
                'original_input': user_input,
                'user_context': user_context,
                'conversation_history': self.conversation_memory[-3:],
                'timestamp': datetime.now().isoformat()
            }
            
            # Generate intelligent response
            response_text = await self._generate_accurate_response(user_input, intent, emotion, context_data)
            
            # Determine action requirement
            action_required = self._requires_action(intent, user_input)
            
            # Security assessment
            security_level = self._assess_security_level(intent, user_input)
            
            # Create response
            ai_response = AIResponse(
                text=response_text,
                emotion=self._determine_response_emotion(intent, emotion),
                intent=intent,
                confidence=confidence,
                context_data=context_data,
                action_required=action_required,
                security_level=security_level,
                processing_time=time.time() - start_time
            )
            
            # Store conversation
            self._store_conversation(user_input, ai_response)
            
            return ai_response
            
        except Exception as e:
            logger.error(f"AI processing error: {e}")
            return AIResponse(
                text="I apologize, Sir. I encountered a processing error. Please try rephrasing your request.",
                emotion=EmotionalState.APOLOGETIC,
                intent=IntentType.CONVERSATION,
                confidence=0.5,
                context_data={'error': str(e)},
                processing_time=time.time() - start_time
            )
    
    async def _generate_accurate_response(self, user_input: str, intent: IntentType, 
                                        emotion: EmotionalState, context: Dict) -> str:
        """Generate accurate, context-aware responses"""
        
        # Handle specific intents with accurate responses
        if intent == IntentType.EMERGENCY:
            return "Emergency situation detected! I'm immediately accessing emergency protocols. Please stay calm while I coordinate assistance."
        
        elif intent == IntentType.EXIT_COMMAND:
            return "Exiting voice mode. Returning to text input interface."
        
        elif intent == IntentType.TIME_QUERY:
            # Use HumanJARVIS for time queries
            return self.human_jarvis.get_time()
        
        elif intent == IntentType.IDENTITY_QUERY:
            import random
            return random.choice(self.personality_responses['identity'])
        
        elif intent == IntentType.WEATHER_QUERY:
            import random
            return random.choice(self.personality_responses['weather_response'])
        
        elif intent == IntentType.SYSTEM_STATUS:
            return self._generate_system_status_response()
        
        elif intent == IntentType.PHONE_CONTROL:
            return self._generate_phone_control_response(user_input)
        
        elif intent == IntentType.SYSTEM_CONTROL:
            return self._generate_system_control_response(user_input)
        
        elif intent == IntentType.CONVERSATION:
            return self._generate_conversation_response(emotion)
        
        else:
            return "I understand your request, Sir. Let me help you with that right away using my enhanced capabilities."
    
    def _generate_system_status_response(self) -> str:
        """Generate system status response with actual data"""
        if SYSTEM_MONITOR_AVAILABLE:
            try:
                cpu_percent = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                
                tars_status = "TARS voice system active" if self.human_jarvis.tars_enabled else "Standard TTS active"
                
                return (f"System status report, Sir: CPU usage at {cpu_percent:.1f}%, "
                       f"Memory at {memory.percent:.1f}% with {memory.available/1024**3:.1f}GB available, "
                       f"disk usage at {disk.percent:.1f}% with {disk.free/1024**3:.1f}GB free space. "
                       f"Voice system: {tars_status}.")
            except Exception as e:
                return f"System monitoring temporarily unavailable. Core systems operational. Voice: {'TARS active' if self.human_jarvis.tars_enabled else 'Standard TTS'}"
        else:
            tars_status = "TARS voice system active" if self.human_jarvis.tars_enabled else "Standard TTS active"
            return f"System monitoring requires psutil library. Core JARVIS systems are operational and ready to serve. Voice system: {tars_status}."
    
    def _generate_phone_control_response(self, user_input: str) -> str:
        """Generate phone control response"""
        user_lower = user_input.lower()
        
        if 'call' in user_lower:
            return "I'll help you initiate that call. Please specify the contact or number."
        elif 'text' in user_lower or 'message' in user_lower:
            return "Opening messaging interface for you, Sir."
        elif 'screenshot' in user_lower:
            return "Taking a screenshot now. The image will be saved to your gallery."
        else:
            return "I'll execute that phone command for you, Sir."
    
    def _generate_system_control_response(self, user_input: str) -> str:
        """Generate system control response"""
        user_lower = user_input.lower()
        
        if 'volume' in user_lower:
            return "Adjusting system volume as requested, Sir."
        elif 'brightness' in user_lower:
            return "Modifying display brightness to your preference."
        elif 'open' in user_lower or 'launch' in user_lower:
            return "Opening the requested application now."
        else:
            return "Executing system command, Sir."
    
    def _generate_conversation_response(self, emotion: EmotionalState) -> str:
        """Generate natural conversation response"""
        responses = {
            EmotionalState.HAPPY: [
                "That's wonderful to hear, Sir! I'm pleased you're having a positive experience.",
                "Excellent! Your enthusiasm is quite uplifting.",
                "I'm delighted to see you in such good spirits today."
            ],
            EmotionalState.CONCERNED: [
                "I understand your concern, Sir. I'm here to help you work through this.",
                "I can sense your worry. Rest assured, we'll address this together.",
                "Your concern is completely valid. How may I assist you with this matter?"
            ],
            EmotionalState.CURIOUS: [
                "That's an intriguing question! I'd be happy to explore this with you.",
                "Your curiosity is admirable, Sir. Let me help satisfy that inquiry.",
                "Fascinating topic! I'm ready to dive into this subject with you."
            ]
        }
        
        if emotion in responses:
            import random
            return random.choice(responses[emotion])
        
        # Default responses
        default_responses = [
            "I'm here and ready to assist you with enhanced TARS voice capability, Sir. How may I be of service?",
            "Certainly, Sir. I'm listening and prepared to help with whatever you need using my improved systems.",
            "I'm at your service with enhanced voice synthesis. What would you like to explore or accomplish today?",
            "Standing by and ready to assist with full TARS voice integration. What can I do for you, Sir?"
        ]
        
        import random
        return random.choice(default_responses)
    
    def _determine_response_emotion(self, intent: IntentType, user_emotion: EmotionalState) -> EmotionalState:
        """Determine appropriate response emotion"""
        emotion_mapping = {
            IntentType.EMERGENCY: EmotionalState.CONCERNED,
            IntentType.TIME_QUERY: EmotionalState.CONFIDENT,
            IntentType.IDENTITY_QUERY: EmotionalState.CONFIDENT,
            IntentType.SYSTEM_STATUS: EmotionalState.ANALYTICAL,
            IntentType.WEATHER_QUERY: EmotionalState.HELPFUL,
            IntentType.CONVERSATION: user_emotion if user_emotion != EmotionalState.NEUTRAL else EmotionalState.FRIENDLY
        }
        
        return emotion_mapping.get(intent, EmotionalState.CONFIDENT)
    
    def _requires_action(self, intent: IntentType, user_input: str) -> bool:
        """Determine if action is required"""
        action_intents = {
            IntentType.PHONE_CONTROL,
            IntentType.SYSTEM_CONTROL,
            IntentType.SYSTEM_STATUS,
            IntentType.EMERGENCY
        }
        return intent in action_intents
    
    def _assess_security_level(self, intent: IntentType, user_input: str) -> SecurityLevel:
        """Assess security level"""
        if intent == IntentType.EMERGENCY:
            return SecurityLevel.TOP_SECRET
        elif intent in [IntentType.SYSTEM_CONTROL, IntentType.SYSTEM_STATUS]:
            return SecurityLevel.CONFIDENTIAL
        elif intent in [IntentType.PHONE_CONTROL]:
            return SecurityLevel.PROTECTED
        else:
            return SecurityLevel.PUBLIC
    
    def _store_conversation(self, user_input: str, ai_response: AIResponse):
        """Store conversation in memory"""
        conversation_entry = {
            'timestamp': datetime.now().isoformat(),
            'user_input': user_input,
            'intent': ai_response.intent.value,
            'emotion': ai_response.emotion.value,
            'confidence': ai_response.confidence,
            'response_text': ai_response.text
        }
        
        self.conversation_memory.append(conversation_entry)
        
        # Keep memory manageable
        if len(self.conversation_memory) > 100:
            self.conversation_memory = self.conversation_memory[-50:]

class ImprovedJARVIS:
    """Improved JARVIS system with TARS voice integration and better accuracy"""
    
    def __init__(self, data_dir: str = None):
        if data_dir is None:
            data_dir = Path.home() / ".jarvis_improved"
        self.data_dir = Path(data_dir)
        os.makedirs(self.data_dir, exist_ok=True)
        
        print("ü§ñ Initializing Enhanced JARVIS System v2.1 with TARS Voice")
        print("=" * 60)
        
        # Initialize components
        print("üß† Loading Enhanced AI Brain with TARS integration...")
        self.brain = AccurateAIBrain(self.data_dir)
        
        print("üéµ Loading Improved Voice System with TARS voice...")
        self.voice = ImprovedVoiceSystem()
        
        # System state
        self.is_running = False
        self.current_user = "Sir"  # Default user address
        self.interaction_count = 0
        self.session_start_time = time.time()
        self.voice_mode = False
        
        print("‚úÖ Enhanced JARVIS System with TARS voice initialized successfully!")
        self._display_system_status()
    
    def _display_system_status(self):
        """Display system status"""
        print(f"\nüìä System Status:")
        print(f"   üß† AI Brain: ‚úÖ Enhanced Accuracy Mode with TARS Integration")
        
        if TARS_VOICE_AVAILABLE and self.brain.human_jarvis.tars_enabled:
            print(f"   üé≠ Voice System: ‚úÖ TARS Voice Active")
        elif self.voice.voice_enabled:
            print(f"   üéµ Voice System: ‚úÖ Standard TTS Available (TARS not found)")
        else:
            print(f"   üéµ Voice System: ‚ùå Limited (Text only)")
            
        print(f"   üîç Intent Recognition: ‚úÖ Improved Patterns")
        print(f"   üì± Platform: {platform.system()} {platform.release()}")
        
        if SYSTEM_MONITOR_AVAILABLE:
            try:
                cpu_percent = psutil.cpu_percent()
                memory = psutil.virtual_memory()
                print(f"   üíª CPU: {cpu_percent:.1f}%")
                print(f"   üß† RAM: {memory.percent:.1f}% ({memory.available/1024**3:.1f}GB available)")
            except:
                pass
    
    async def process_and_respond(self, user_input: str, voice_input: bool = False) -> Dict[str, Any]:
        """Process input and generate response"""
        start_time = time.time()
        self.interaction_count += 1
        
        print(f"\n--- Interaction #{self.interaction_count} ---")
        print(f"üë§ {'üé§ ' if voice_input else ''}User: {user_input}")
        
        try:
            # Process with AI brain
            user_context = {
                'username': self.current_user,
                'voice_input': voice_input,
                'interaction_count': self.interaction_count,
                'session_duration': time.time() - self.session_start_time,
                'tars_voice_available': self.brain.human_jarvis.tars_enabled
            }
            
            brain_response = await self.brain.process_input(user_input, user_context)
            
            # Check for exit command
            if brain_response.intent == IntentType.EXIT_COMMAND:
                self.voice_mode = False
                await self.voice.speak(brain_response.text, brain_response.emotion)
                return {
                    "brain_response": asdict(brain_response),
                    "exit_voice_mode": True,
                    "interaction_number": self.interaction_count
                }
            
            # Speak the response with TARS voice priority
            speech_success = await self.voice.speak(brain_response.text, brain_response.emotion)
            
            # Calculate processing time
            processing_time = time.time() - start_time
            
            # Display response with voice system indicator
            voice_indicator = "üé≠ TARS" if self.brain.human_jarvis.tars_enabled else "üîä TTS" if speech_success else "üí¨ Text"
            print(f"ü§ñ JARVIS ({voice_indicator} | {brain_response.emotion.value}): {brain_response.text}")
            print(f"üìä Stats: Intent={brain_response.intent.value}, Confidence={brain_response.confidence:.2f}, Time={processing_time:.2f}s")
            
            return {
                "brain_response": asdict(brain_response),
                "speech_success": speech_success,
                "processing_time": processing_time,
                "interaction_number": self.interaction_count,
                "tars_voice_used": self.brain.human_jarvis.tars_enabled
            }
            
        except Exception as e:
            logger.error(f"Processing error: {e}")
            
            error_message = "I encountered a technical difficulty, Sir. All core systems including TARS voice remain operational."
            await self.voice.speak(error_message, EmotionalState.CONCERNED)
            
            return {
                "error": str(e),
                "processing_time": time.time() - start_time,
                "interaction_number": self.interaction_count
            }
    
    async def voice_conversation_mode(self):
        """Improved voice conversation mode with TARS voice"""
        print("\nüéôÔ∏è  Enhanced Voice Conversation Mode with TARS Voice")
        print("=" * 60)
        print("‚ú® Improvements:")
        print("  ‚Ä¢ TARS voice integration for cinematic experience")
        print("  ‚Ä¢ Better speech recognition accuracy")
        print("  ‚Ä¢ Improved intent detection")
        print("  ‚Ä¢ More natural responses")
        print("  ‚Ä¢ Enhanced error handling")
        print("\nüìù Commands:")
        print("  ‚Ä¢ Say 'Hey JARVIS' followed by your command")
        print("  ‚Ä¢ Say 'Exit voice mode' to return to text")
        print("=" * 60)
        
        if not self.voice.voice_enabled:
            print("‚ùå Voice input not available. Please install required packages:")
            print("   pip install SpeechRecognition pyaudio pyttsx3")
            return
        
        self.voice_mode = True
        
        tts_message = "Enhanced voice conversation mode with TARS voice activated. I'm listening for 'Hey JARVIS' followed by your command."
        if not self.brain.human_jarvis.tars_enabled:
            tts_message = "Enhanced voice conversation mode activated with standard TTS. I'm listening for 'Hey JARVIS' followed by your command."
        
        await self.voice.speak(tts_message, EmotionalState.CONFIDENT)
        
        consecutive_failures = 0
        max_failures = 5
        
        while self.voice_mode and self.is_running:
            try:
                # Listen for wake word
                voice_system = "TARS Voice" if self.brain.human_jarvis.tars_enabled else "Standard TTS"
                print(f"\nüéß Listening for 'Hey JARVIS'... ({voice_system} | Failures: {consecutive_failures}/{max_failures})")
                audio_input = await self.voice.listen(timeout=10)
                
                if audio_input:
                    audio_lower = audio_input.lower().strip()
                    
                    # Check for exit commands
                    if "exit voice mode" in audio_lower or "quit voice mode" in audio_lower:
                        await self.voice.speak("Exiting voice mode. Returning to text input.", EmotionalState.NEUTRAL)
                        self.voice_mode = False
                        break
                    
                    # Check for wake word
                    elif "hey jarvis" in audio_lower or "jarvis" in audio_lower:
                        await self.voice.speak("Yes, Sir?", EmotionalState.FOCUSED)
                        
                        # Listen for actual command
                        print("üé§ Listening for command...")
                        command = await self.voice.listen(timeout=8)
                        
                        if command:
                            result = await self.process_and_respond(command, voice_input=True)
                            
                            # Check if we should exit voice mode
                            if result.get("exit_voice_mode", False):
                                break
                            
                            consecutive_failures = 0  # Reset failure count on success
                            
                            # Small pause for natural conversation flow
                            await asyncio.sleep(0.5)
                        else:
                            consecutive_failures += 1
                            await self.voice.speak("I didn't catch that. Please try again.", EmotionalState.CONCERNED)
                    else:
                        # Didn't hear wake word clearly
                        consecutive_failures += 1
                        if consecutive_failures >= 3:
                            await self.voice.speak("I'm having trouble hearing the wake word. Please say 'Hey JARVIS' clearly.", EmotionalState.CONCERNED)
                else:
                    consecutive_failures += 1
                
                # Check if too many consecutive failures
                if consecutive_failures >= max_failures:
                    await self.voice.speak("Voice recognition experiencing difficulties. Exiting voice mode.", EmotionalState.CONCERNED)
                    self.voice_mode = False
                    break
                
                # Small delay to prevent excessive CPU usage
                await asyncio.sleep(0.1)
                
            except KeyboardInterrupt:
                print("\nüõë Voice mode interrupted by user.")
                await self.voice.speak("Voice mode interrupted.", EmotionalState.NEUTRAL)
                self.voice_mode = False
                break
            except Exception as e:
                logger.error(f"Voice mode error: {e}")
                await self.voice.speak("Voice system error. Continuing in text mode.", EmotionalState.CONCERNED)
                self.voice_mode = False
                break
    
    async def conversation_loop(self):
        """Main conversation loop with improved handling and TARS voice"""
        print("\nüé≠ Improved JARVIS Conversation Mode with TARS Voice Integration")
        print("=" * 80)
        print("üöÄ Enhanced Features:")
        print("  ‚Ä¢ TARS voice system for cinematic AI experience")
        print("  ‚Ä¢ More accurate intent recognition")
        print("  ‚Ä¢ Better response generation")
        print("  ‚Ä¢ Improved voice interaction")
        print("  ‚Ä¢ Enhanced error handling")
        print("\nüìù Commands:")
        print("  ‚Ä¢ Type your message to talk with JARVIS")
        print("  ‚Ä¢ Type 'voice mode' to switch to voice input")
        print("  ‚Ä¢ Type 'time' to get current time")
        print("  ‚Ä¢ Type 'who are you' to learn about JARVIS")
        print("  ‚Ä¢ Type 'system status' to see system information")
        print("  ‚Ä¢ Type 'exit', 'quit', or 'goodbye' to end")
        print("=" * 80)
        
        self.is_running = True
        
        # Welcome message with TARS voice indication
        welcome_msg = "Good day, Sir! Enhanced JARVIS system with TARS voice integration is ready. I have improved accuracy and better understanding of your requests."
        if not self.brain.human_jarvis.tars_enabled:
            welcome_msg = "Good day, Sir! Enhanced JARVIS system is ready with standard voice synthesis. I have improved accuracy and better understanding of your requests."
        
        await self.voice.speak(welcome_msg, EmotionalState.CONFIDENT)
        
        while self.is_running:
            try:
                session_time = time.time() - self.session_start_time
                voice_status = "TARS" if self.brain.human_jarvis.tars_enabled else "TTS"
                print(f"\n[Session: {self.interaction_count} interactions | Duration: {session_time/60:.1f}min | Voice: {voice_status}]")
                user_input = input("üë§ You: ").strip()
                
                if not user_input:
                    continue
                
                # Handle special commands
                user_lower = user_input.lower()
                
                if user_lower in ['exit', 'quit', 'goodbye', 'bye']:
                    await self.voice.speak("Goodbye! Enhanced JARVIS system with TARS voice shutting down. Have a wonderful day!", EmotionalState.CARING)
                    await self.shutdown()
                    break
                
                elif user_lower == 'voice mode':
                    await self.voice_conversation_mode()
                    continue
                
                elif user_lower in ['help', '?']:
                    await self._show_help()
                    continue
                
                elif user_lower == 'stats':
                    await self._show_stats()
                    continue
                
                elif user_lower == 'tars status':
                    await self._show_tars_status()
                    continue
                
                # Process normal conversation
                result = await self.process_and_respond(user_input)
                
                # Handle any special results
                if "error" in result:
                    print(f"‚ö†Ô∏è  Processing error occurred")
                
            except KeyboardInterrupt:
                print("\nüõë Conversation interrupted by user.")
                await self.voice.speak("Conversation interrupted. JARVIS with TARS voice standing by.", EmotionalState.NEUTRAL)
                break
            except Exception as e:
                logger.error(f"Conversation loop error: {e}")
                print(f"‚ùå An error occurred: {e}")
                await self.voice.speak("System error detected. Attempting recovery with all voice systems.", EmotionalState.CONCERNED)
                continue
    
    async def _show_help(self):
        """Show help information"""
        print("\n‚ùì Enhanced JARVIS Help with TARS Voice Integration")
        print("=" * 60)
        
        print("üó£Ô∏è  Natural Conversation:")
        print("   ‚Ä¢ Ask questions: 'What time is it?', 'Who are you?'")
        print("   ‚Ä¢ Get system info: 'System status', 'Hardware details'")
        print("   ‚Ä¢ Weather queries: 'What's the weather today?'")
        
        print("\nüé§ Voice Commands:")
        print("   ‚Ä¢ 'voice mode' - Enable voice input with TARS voice")
        print("   ‚Ä¢ Say 'Hey JARVIS' + your command")
        print("   ‚Ä¢ 'Exit voice mode' - Return to text")
        
        print("\nüîß System Commands:")
        print("   ‚Ä¢ 'help' - Show this help")
        print("   ‚Ä¢ 'stats' - Show session statistics")
        print("   ‚Ä¢ 'tars status' - Show TARS voice system status")
        print("   ‚Ä¢ 'exit' - Shutdown JARVIS")
        
        print("\n‚ú® New in v2.1 with TARS Integration:")
        print("   ‚Ä¢ TARS voice system for cinematic AI experience")
        print("   ‚Ä¢ Better intent recognition accuracy")
        print("   ‚Ä¢ More natural response generation")
        print("   ‚Ä¢ Enhanced voice interaction")
        print("   ‚Ä¢ Improved error handling")
        
        await self.voice.speak("Help information displayed. I'm ready to assist with TARS voice integration and improved accuracy.", EmotionalState.CONFIDENT)
    
    async def _show_stats(self):
        """Show session statistics"""
        session_duration = time.time() - self.session_start_time
        
        print(f"\nüìä Enhanced JARVIS Session Statistics")
        print("=" * 60)
        print(f"‚è±Ô∏è  Session Duration: {session_duration/60:.1f} minutes")
        print(f"üí¨ Total Interactions: {self.interaction_count}")
        print(f"üé≠ TARS Voice System: {'‚úÖ Active' if self.brain.human_jarvis.tars_enabled else '‚ùå Inactive'}")
        print(f"üé§ Standard Voice System: {'‚úÖ Available' if self.voice.voice_enabled else '‚ùå Limited'}")
        print(f"üß† Memory Entries: {len(self.brain.conversation_memory)}")
        
        if SYSTEM_MONITOR_AVAILABLE:
            try:
                cpu = psutil.cpu_percent()
                memory = psutil.virtual_memory()
                print(f"üíª System Load: CPU {cpu:.1f}%, RAM {memory.percent:.1f}%")
            except:
                print("üíª System monitoring: Limited")
        
        # Show recent intents
        if self.brain.conversation_memory:
            recent_intents = [entry['intent'] for entry in self.brain.conversation_memory[-5:]]
            from collections import Counter
            intent_counts = Counter(recent_intents)
            print(f"\nüéØ Recent Intent Distribution:")
            for intent, count in intent_counts.most_common():
                print(f"   ‚Ä¢ {intent}: {count}")
        
        voice_msg = "Session statistics displayed. Enhanced JARVIS with TARS voice is performing optimally." if self.brain.human_jarvis.tars_enabled else "Session statistics displayed. Enhanced JARVIS is performing optimally with standard voice."
        await self.voice.speak(voice_msg, EmotionalState.ANALYTICAL)
    
    async def _show_tars_status(self):
        """Show TARS voice system status"""
        print(f"\nüé≠ TARS Voice System Status")
        print("=" * 60)
        
        if TARS_VOICE_AVAILABLE and self.brain.human_jarvis.tars_enabled:
            print(f"üîä TARS Voice Module: ‚úÖ Active")
            print("   ‚Ä¢ TARS voice synthesis: Operational")
            print("   ‚Ä¢ Cinematic AI experience: Enabled")
            print("   ‚Ä¢ Voice quality: Enhanced")
            await self.voice.speak("TARS voice system is fully operational and providing enhanced audio experience.", EmotionalState.CONFIDENT)
        elif TARS_VOICE_AVAILABLE:
            print(f"üîä TARS Voice Module: ‚ö†Ô∏è Available but inactive")
            print("   ‚Ä¢ TARS voice synthesis: Available but not initialized")
            print("   ‚Ä¢ Fallback TTS: Active")
            await self.voice.speak("TARS voice system is available but not currently active. Using standard text-to-speech.", EmotionalState.NEUTRAL)
        else:
            print(f"üîä TARS Voice Module: ‚ùå Not Available")
            print("   ‚Ä¢ TARS voice synthesis: Module not found")
            print("   ‚Ä¢ Fallback TTS: Active")
            print("   ‚Ä¢ Issue: tars_voice_player module not installed")
            print("\nüì¶ To install TARS voice:")
            print("   ‚Ä¢ Place tars_voice_player.py in ../voice_cloning/ directory")
            print("   ‚Ä¢ Install required TARS voice dependencies")
            await self.voice.speak("TARS voice system is not available. Module not found. Using standard text-to-speech as fallback.", EmotionalState.CONCERNED)
        
        print("=" * 60)
    
    async def shutdown(self):
        """Graceful system shutdown"""
        print("\nüîÑ Shutting down Enhanced JARVIS System with TARS Integration...")
        
        try:
            # Save conversation data
            print("üíæ Saving enhanced conversation data...")
            
            # Create session summary
            session_duration = time.time() - self.session_start_time
            session_summary = {
                'session_start': datetime.fromtimestamp(self.session_start_time).isoformat(),
                'session_end': datetime.now().isoformat(),
                'duration_minutes': session_duration / 60,
                'total_interactions': self.interaction_count,
                'conversation_memory': self.brain.conversation_memory,
                'system_info': {
                    'platform': platform.system(),
                    'python_version': platform.python_version(),
                    'voice_enabled': self.voice.voice_enabled,
                    'tars_voice_enabled': self.brain.human_jarvis.tars_enabled
                }
            }
            
            # Save to file
            summary_file = self.data_dir / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(summary_file, 'w') as f:
                json.dump(session_summary, f, indent=2, default=str)
            
            print(f"üíæ Session data saved to: {summary_file}")
            
            # Final report
            print(f"\nüìä Final Session Report:")
            print(f"   Duration: {session_duration/60:.1f} minutes")
            print(f"   Interactions: {self.interaction_count}")
            print(f"   TARS Voice: {'Used' if self.brain.human_jarvis.tars_enabled else 'Not Available'}")
            print(f"   Standard Voice: {'Available' if self.voice.voice_enabled else 'Text Only'}")
            
            self.is_running = False
            print("‚úÖ Enhanced JARVIS system with TARS voice integration shutdown complete.")
            
        except Exception as e:
            logger.error(f"Shutdown error: {e}")
            print(f"‚ö†Ô∏è  Shutdown completed with warnings: {e}")
    
    async def run(self):
        """Main application entry point"""
        try:
            # Display welcome banner
            await self._display_welcome_banner()
            
            # Start conversation loop
            await self.conversation_loop()
            
        except Exception as e:
            logger.error(f"Main application error: {e}")
            print(f"üí• Fatal error: {e}")
            await self.voice.speak("Critical system error. Enhanced JARVIS with TARS voice shutting down.", EmotionalState.CONCERNED)
        finally:
            if self.is_running:
                await self.shutdown()
    
    async def _display_welcome_banner(self):
        """Display welcome banner"""
        banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                ü§ñ Enhanced J.A.R.V.I.S. System v2.1 + TARS Voice           ‚ïë
‚ïë                     Just A Rather Very Intelligent System                    ‚ïë
‚ïë                                                                              ‚ïë
‚ïë  üé≠ TARS Voice Integration  üß† Enhanced AI Brain    üéµ Superior Audio       ‚ïë
‚ïë  üîç Smart Intent Detection   üìä Better Analytics   üöÄ Production Ready      ‚ïë
‚ïë                                                                              ‚ïë
‚ïë                        ‚ú® Key Improvements ‚ú®                               ‚ïë
‚ïë  ‚Ä¢ Cinematic TARS voice system for enhanced user experience                 ‚ïë
‚ïë  ‚Ä¢ More accurate speech recognition and intent detection                     ‚ïë
‚ïë  ‚Ä¢ Enhanced natural language understanding                                   ‚ïë
‚ïë  ‚Ä¢ Better error handling and recovery                                        ‚ïë
‚ïë  ‚Ä¢ Improved conversation flow and responses                                  ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        """
        print(banner)
        
        welcome_message = "Enhanced J.A.R.V.I.S. System Version 2.1 with TARS voice integration initialized. Just A Rather Very Intelligent System with cinematic voice experience and improved accuracy."
        if not self.brain.human_jarvis.tars_enabled:
            welcome_message = "Enhanced J.A.R.V.I.S. System Version 2.1 initialized with standard voice synthesis. TARS voice module not available but all other systems operational."
        
        await self.voice.speak(welcome_message, EmotionalState.CONFIDENT)

def check_dependencies():
    """Check and report on dependencies including TARS voice"""
    print("üîç Enhanced JARVIS Dependency Check with TARS Integration")
    print("=" * 60)
    
    deps = {
        "Core Python": (True, f"Python {platform.python_version()}"),
        "TARS Voice System": (TARS_VOICE_AVAILABLE, "tars_voice_player module"),
        "Speech Recognition": (SPEECH_AVAILABLE, "SpeechRecognition library"),
        "Text-to-Speech": (TTS_AVAILABLE, "pyttsx3 library"),
        "System Monitoring": (SYSTEM_MONITOR_AVAILABLE, "psutil library"),
        "GPU Monitoring": (GPU_MONITOR_AVAILABLE, "GPUtil library"),
        "Encryption": (True, "cryptography library")
    }
    
    for name, (available, desc) in deps.items():
        status = "‚úÖ" if available else "‚ùå"
        print(f"   {status} {name}: {desc}")
    
    if not TARS_VOICE_AVAILABLE:
        print("\nüé≠ To enable TARS voice functionality:")
        print("   ‚Ä¢ Create or obtain tars_voice_player.py")
        print("   ‚Ä¢ Place it in ../voice_cloning/ directory relative to this script")
        print("   ‚Ä¢ Ensure all TARS voice dependencies are installed")
        print("   ‚Ä¢ The system will work with standard TTS as fallback")
    
    if not SPEECH_AVAILABLE or not TTS_AVAILABLE:
        print("\nüì¶ To enable full voice functionality, install:")
        print("   pip install SpeechRecognition pyttsx3 pyaudio")
    
    if not SYSTEM_MONITOR_AVAILABLE:
        print("\nüì¶ To enable system monitoring, install:")
        print("   pip install psutil")
    
    print("=" * 60)
    return True

async def main():
    """Main entry point for Enhanced JARVIS with TARS Voice"""
    try:
        # Check dependencies
        check_dependencies()
        
        # Initialize Enhanced JARVIS with TARS integration
        jarvis = ImprovedJARVIS()
        
        # Run the application
        await jarvis.run()
        
    except KeyboardInterrupt:
        print("\nüõë Enhanced JARVIS with TARS voice interrupted by user.")
    except Exception as e:
        logger.error(f"Fatal application error: {e}")
        print(f"üí• Fatal error occurred: {e}")

if __name__ == "__main__":
    """
    Enhanced JARVIS System v2.1 with TARS Voice Integration
    
    Key Improvements:
    - TARS voice system integration for cinematic AI experience
    - Better speech recognition accuracy
    - Improved intent detection patterns
    - More natural response generation
    - Enhanced error handling
    - Better conversation flow
    - Accurate time and system status responses
    
    Installation:
    pip install asyncio cryptography requests pathlib
    
    Optional (for full functionality):
    pip install SpeechRecognition pyttsx3 pyaudio psutil GPUtil
    
    TARS Voice Requirements:
    - tars_voice_player.py in ../voice_cloning/ directory
    - TARS voice model and dependencies
    
    Usage:
    python improved_jarvis_tars.py
    
    Features:
    - Enhanced natural language understanding
    - TARS voice integration for superior audio experience
    - Improved voice interaction accuracy
    - Better system integration
    - More reliable error handling
    - Accurate time and weather responses
    - Cinematic AI assistant experience
    """
    
    print("üöÄ Starting Enhanced JARVIS System v2.1 with TARS Voice Integration...")
    print("=" * 80)
    
    # Run the application
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"üí• Failed to start Enhanced JARVIS with TARS voice: {e}")
        logger.error(f"Application startup failed: {e}")
        sys.exit(1)
    
    print("üîö Enhanced JARVIS session with TARS voice integration completed")